# Convolutional Network 

## What inspired Convolutional Networks?

CNNs are biologically-inspired models inspired by research by D. H. Hubel and T. N. Wiesel. They proposed an explanation for the way in which mammals visually perceive the world around them using a layered architecture of neurons in the brain, and this in turn inspired engineers to attempt to develop similar pattern recognition mechanisms in computer vision.


In their hypothesis, within the visual cortex, complex functional responses generated by "complex cells" are constructed from more simplistic responses from "simple cells'. 

For instances, simple cells would respond to oriented edges etc, while complex cells will also respond to oriented edges but with a degree of spatial invariance.

Receptive fields exist for cells, where a cell responds to a summation of inputs from other local cells.

The architecture of deep convolutional neural networks was inspired by the ideas mentioned above 
- local connections 
- layering  
- spatial invariance (shifting the input signal results in an equally shifted output signal. , most of us are able to recognize specific faces under a variety of conditions because we learn abstraction These abstractions are thus invariant to size, contrast, rotation, orientation
 
However, it remains to be seen if these computational mechanisms of convolutional neural networks are similar to the computation mechanisms occurring in the primate visual system

- convolution operation
- shared weights
- pooling/subsampling 

## How does it work? 

![alt text](https://images.nature.com/w926/nature-assets/srep/2016/160610/srep27755/images_hires/srep27755-f1.jpg "Logo Title Text 1")
![alt text](https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1497876372993.jpg "Logo Title Text 1")
### Step 1 - Prepare a dataset of images

![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure1.png "Logo Title Text 1")

- Every image is a matrix of pixel values. 
- The range of values that can be encoded in each pixel depends upon its bit size. 
- Most commonly, we have 8 bit or 1 Byte-sized pixels. Thus the possible range of values a single pixel can represent is [0, 255]. 
- However, with coloured images, particularly RGB (Red, Green, Blue)-based images, the presence of separate colour channels (3 in the case of RGB images) introduces an additional ‘depth’ field to the data, making the input 3-dimensional. 
- Hence, for a given RGB image of size, say 255×255 (Width x Height) pixels, we’ll have 3 matrices associated with each image, one for each of the colour channels. 
- Thus the image in it’s entirety, constitutes a 3-dimensional structure called the Input Volume (255x255x3).

<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/1.png?raw=true'>

* imput features

<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/2.png?raw=true'>

### Step 2 - Convolution 
### How Filter works in convolution
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/3.png?raw=true'>
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/4.png?raw=true'>
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/5.png?raw=true'>

### Various Types of Filters
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/6.png?raw=true'>
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/11.png?raw=true'>
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/12.png?raw=true'>

### Padding
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/7.png?raw=true'>

<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/8.png?raw=true'>

### Strid
<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/9.png?raw=true'>

<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/10.png?raw=true'>

<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/11.png?raw=true'>

<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/12.png?raw=true'>





![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/more_images/Convolution_schematic.gif "Logo Title Text 1")

![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_2.png "Logo Title Text 1")

- A convolution is an orderly procedure where two sources of information are intertwined.

- A kernel (also called a filter) is a smaller-sized matrix in comparison to the input dimensions of the image, that consists of real valued entries.

- Kernels are then convolved with the input volume to obtain so-called ‘activation maps’ (also called feature maps).  
- Activation maps indicate ‘activated’ regions, i.e. regions where features specific to the kernel have been detected in the input. 

- The real values of the kernel matrix change with each learning iteration over the training set, indicating that the network is learning to identify which regions are of significance for extracting features from the data.

- We compute the dot product between the kernel and the input matrix. -The convolved value obtained by summing the resultant terms from the dot product forms a single entry in the activation matrix. 

- The patch selection is then slided (towards the right, or downwards when the boundary of the matrix is reached) by a certain amount called the ‘stride’ value, and the process is repeated till the entire input image has been processed. - The process is carried out for all colour channels.

- instead of connecting each neuron to all possible pixels, we specify a 2 dimensional region called the ‘receptive field[14]’ (say of size 5×5 units) extending to the entire depth of the input (5x5x3 for a 3 colour channel input), within which the encompassed pixels are fully connected to the neural network’s input layer. It’s over these small regions that the network layer cross-sections (each consisting of several neurons (called ‘depth columns’)) operate and produce the activation map. (reduces computational complexity)

![alt text](http://i.imgur.com/g4hRI6Z.png "Logo Title Text 1")
![alt text](http://i.imgur.com/tpQvMps.jpg "Logo Title Text 1")
![alt text](http://i.imgur.com/oyXkhHi.jpg "Logo Title Text 1")
![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_5.png "Logo Title Text 1")

Great resource on description of  convolution (discrete vs continous)  & the fourier transform

http://timdettmers.com/2015/03/26/convolution-deep-learning/


###  Step 3 - Pooling
![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_6.png "Logo Title Text 1")

- Pooling reducing the spatial dimensions (Width x Height) of the Input Volume for the next Convolutional Layer. It does not affect the depth dimension of the Volume.  
- The transformation is either performed by taking the maximum value from the values observable in the window (called ‘max pooling’), or by taking the average of the values. Max pooling has been favoured over others due to its better performance characteristics.
- also called downsampling

###  Step 4 - Normalization (ReLU in our case) 


<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/13.png?raw=true'>

<img src='https://github.com/iAmKankan/Deep-Learning/blob/master/CNN_pic/14.png?raw=true'>

On the 4x4 matrix there applies an non-linearity (ReLu) and a bias (which is a real number) and that gives a final 4X4 matrix. and the end result is 4X4X2 coz we have used 2 filters if we had 10 filters it would be 4X4X10





![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/CodeCogsEqn-3.png "Logo Title Text 1")

Normalization (keep the math from breaking by turning all negative numbers to 0)  (RELU) a stack of images becomes a stack of images with no negative values. 

Repeat Steps 2-4 several times. More, smaller images (feature maps created at every layer)

### Step 5 - Regularization 

- Dropout forces an artificial neural network to learn multiple independent representations of the same data by alternately randomly disabling neurons in the learning phase.
- Dropout is a vital feature in almost every state-of-the-art neural network implementation.
- To perform dropout on a layer, you randomly set some of the layer's values to 0 during forward propagation.

See [this](http://iamtrask.github.io/2015/07/28/dropout/)

![alt text](https://i.stack.imgur.com/CewjH.png "Logo Title Text 1")

###  Step 6 - Probability Conversion

At the very end of our network (the tail), we'll apply a softmax function to convert the outputs to probability values for each class. 

![alt text](https://1.bp.blogspot.com/-FHDU505euic/Vs1iJjXHG0I/AAAAAAABVKg/x4g0FHuz7_A/s1600/softmax.JPG "Logo Title Text 1")


###  Step 7 - Choose most likely label (max probability value) 

argmax(softmax_outputs)

These 7 steps are one forward pass through the network.
## So how do we learn the magic numbers? 

- We can learn features and weight values through backpropagation

![alt text](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/images/cover.png "Logo Title Text 1")

![alt text](https://image.slidesharecdn.com/cnn-toupload-final-151117124948-lva1-app6892/95/convolutional-neural-networks-cnn-52-638.jpg?cb=1455889178 "Logo Title Text 1")

The other hyperparameters are set by humans and they are an active field of research (finding the optimal ones)

i.e -  number of neurons, number of features, size of features, poooling window size, window stride


## When is a good time to use it?

- To classify images
- To generate images (more on that later..)

![alt text](https://nlml.github.io/images/convnet_diagram.png "Logo Title Text 1")

But can also be applied to any any spatial 2D or 3D data. Images. Even sound and text. A rule of thumb is if you data is just as useful if you swap out the rows and columns, like customer data, then you can't use a CNN.


## Good examples

Robot learns to grasp (combining CNNs)

![alt text](https://img.newatlas.com/youtube-robot-6.jpg?auto=format%2Ccompress&fit=max&h=670&q=60&w=1000&s=d003e42afa7e462fd711c6a99f21b51f "Logo Title Text 1")

Tensorflow! https://github.com/upul/CarND-TensorFlow-Lab

Adversarial CNNs https://github.com/michbad/adversarial-mnist
