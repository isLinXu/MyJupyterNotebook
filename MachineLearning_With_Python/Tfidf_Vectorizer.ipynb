{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we need tfidf?\n",
    "* Text Analysis is a major application field for machine learning algorithms. \n",
    "* Raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "* In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "    * **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "    * **counting** the occurrences of tokens in each document.\n",
    "    * **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "#### What are features and samples ?\n",
    "* **feature** = each individual token occurrence frequency (normalized or not).\n",
    "* **sample** = the vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "\n",
    "\n",
    "#### What are vectorization and Bag of Words?\n",
    "* We call vectorization the general process of turning a collection of text documents into numerical feature vectors. \n",
    "* This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. \n",
    "\n",
    "\n",
    "### TF-IDF(term frequency–inverse document frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Term Frequency – Inverse Document Frequency(tf-idf)is a method to evaluate the importance of a specific word in a document.\n",
    "* It converts the textual representation of information into a Vector Space Model (VSM), or into sparse features,\n",
    "* VSM is an algebraic model representing textual information as a vector, the components of this vector could represent the importance of a term (tf–idf) or even the absence or presence (Bag of Words) of it in a document\n",
    "* This is a common term weighting scheme in information retrieval.\n",
    "* The goal is to scale down the impact of tokens that occur very frequently in a given corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=[\n",
    "'The sky is blue',\n",
    "'The sky is not blue',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sky is not blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Text\n",
       "0      The sky is blue\n",
       "1  The sky is not blue"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Text':corpus})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=\"https://github.com/iAmKankan/MachineLearning_With_Python/blob/master/IMG2.jpg?raw=true\">\n",
    "\n",
    "#### Our dataset or corpus:\n",
    "\n",
    "Document a='The sky is blue'     \n",
    "Document b='The sky is not blue'\n",
    "\n",
    "#### Step 1:\n",
    "\n",
    "|   |tf(a)|tf(b)| \n",
    "|---|--|---|---| \n",
    "|the|1|1| \n",
    "|sky|1|1| \n",
    "|is|1|1| \n",
    "|blue|1|1| \n",
    "|not|0|1 \n",
    "\n",
    "#### Step 2:\n",
    "\n",
    "\n",
    "* On a large document the frequency of the terms will be much higher than the smaller ones. Hence we need to normalize the document based on its size\n",
    "\n",
    "|   |tf(a)|tf(b)|N(a)|N(b)| \n",
    "|---|--|---|---|---|---|\n",
    "|the|1|1|1/4|1/5| \n",
    "|sky|1|1|1/4|1/5| \n",
    "|is|1|1|1/4|1/5| \n",
    "|blue|1|1|1/4|1/5| \n",
    "|not|0|1|0|1/5| \n",
    "\n",
    "#### Step 3:\n",
    "\n",
    "\n",
    "* IDF = 1 + loge(Total Number Of Documents / Number Of Documents with term t appear)\n",
    "\n",
    "|   |tf(a)|tf(b)|N(a)|N(b)| IDF|\n",
    "|---|--|---|---|---|---|\n",
    "|the|1|1|1/4|1/5|1+loge(2/2)|\n",
    "|sky|1|1|1/4|1/5|1+loge(2/2)|\n",
    "|is|1|1|1/4|1/5|1+loge(2/2)|\n",
    "|blue|1|1|1/4|1/5|1+loge(2/2)|\n",
    "|not|0|1|0|1/5|1+loge(2/1)|\n",
    "\n",
    "\n",
    "---\n",
    "#### Final Step:\n",
    "\n",
    "* Final tf * idf\n",
    "\n",
    "|   |tf(a)|tf(b)|N(a)|N(b)| IDF|tf * idf|Document a|Document b|\n",
    "|---|--|---|---|---|---|--|\n",
    "|the|1|1|1/4|1/5|1+loge(2/2)|  |.25|.2|\n",
    "|sky|1|1|1/4|1/5|1+loge(2/2)|  |.25|.2|\n",
    "|is|1|1|1/4|1/5|1+loge(2/2)|  |.25|.2|\n",
    "|blue|1|1|1/4|1/5|1+loge(2/2)|  |.25|.2|\n",
    "|not|0|1|0|1/5|1+loge(2/1)|  |0|.1386294|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OR the matrix form:\n",
    "\n",
    "\n",
    "\n",
    "|  | blue |is  |  not| sky |the|\n",
    "|---|--|----|----|---|---|\n",
    "|Document a|.25|.25|0|.25|.25|\n",
    "|Document b|.2|.2|.1386294|.2|.2|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can custom Stop words: \"stop_words = frozenset([list of words we want to restrict])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()#stop_words = frozenset(['sky','the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.5       ,  0.        ,  0.5       ,  0.5       ],\n",
       "       [ 0.4090901 ,  0.4090901 ,  0.57496187,  0.4090901 ,  0.4090901 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit_transform(df.Text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'is', 'not', 'sky', 'the']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.        ,  1.40546511,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Default Stop Words\n",
    "Currently there are 318 words in that frozenset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'never', 'fifteen', 'such', 'because', 'thereby', 'on', 'every', 'rather', 'amongst', 'describe', 'itself', 'cant', 'six', 'once', 'ten', 'whereas', 'themselves', 'amoungst', 'these', 'mostly', 'found', 'if', 'are', 'beforehand', 'him', 'meanwhile', 'done', 'into', 'ever', 'more', 'same', 'hers', 'towards', 'who', 'alone', 'back', 'down', 'along', 'each', 'bottom', 'due', 'from', 'less', 'someone', 'without', 'you', 'moreover', 'those', 'ourselves', 'first', 'further', 'hereby', 'fire', 'had', 'her', 'inc', 'my', 'always', 'hundred', 'seeming', 'get', 're', 'though', 'even', 'con', 'until', 'onto', 'we', 'somehow', 'there', 'sometime', 'i', 'almost', 'whose', 'that', 'yourself', 'fill', 'mill', 'where', 'a', 'seems', 'although', 'none', 'very', 'whatever', 'beyond', 'other', 'while', 'cannot', 'only', 'be', 'thin', 'amount', 'except', 'under', 'detail', 'across', 'whereafter', 'latterly', 'me', 'thus', 'anyone', 'namely', 'twenty', 'everywhere', 'un', 'might', 'not', 'otherwise', 'and', 'few', 'however', 'bill', 'cry', 'seemed', 'put', 'empty', 'which', 'too', 'out', 'side', 'formerly', 'when', 'of', 'whenever', 'last', 'interest', 'than', 'whole', 'four', 'de', 'ours', 'below', 'thereafter', 'another', 'keep', 'after', 'must', 'mine', 'neither', 'himself', 'yours', 'hereafter', 'next', 'am', 'much', 'any', 'everyone', 'nobody', 'somewhere', 'via', 'find', 'sincere', 'therein', 'own', 'give', 'three', 'still', 'whereby', 'whom', 'should', 'for', 'no', 'been', 'etc', 'now', 'again', 'nor', 'she', 'them', 'ltd', 'serious', 'before', 'five', 'all', 'least', 'became', 'name', 'to', 'seem', 'some', 'an', 'sometimes', 'third', 'us', 'wherein', 'herself', 'during', 'its', 'afterwards', 'whence', 'either', 'hasnt', 'perhaps', 'the', 'thence', 'thick', 'could', 'eg', 'by', 'latter', 'made', 'how', 'whoever', 'sixty', 'would', 'may', 'take', 'thereupon', 'since', 'above', 'nowhere', 'eight', 'through', 'throughout', 'being', 'have', 'thru', 'anyhow', 'two', 'so', 'part', 'myself', 'yourselves', 'anything', 'as', 'also', 'both', 'yet', 'with', 'but', 'over', 'elsewhere', 'go', 'something', 'co', 'per', 'show', 'about', 'others', 'together', 'nevertheless', 'nine', 'here', 'beside', 'has', 'will', 'front', 'behind', 'between', 'in', 'was', 'whereupon', 'at', 'is', 'upon', 'do', 'therefore', 'anywhere', 'within', 'besides', 'forty', 'top', 'he', 'off', 'why', 'already', 'often', 'can', 'becomes', 'former', 'our', 'please', 'this', 'become', 'full', 'twelve', 'becoming', 'enough', 'indeed', 'hence', 'or', 'were', 'it', 'system', 'well', 'then', 'see', 'up', 'among', 'many', 'his', 'herein', 'everything', 'nothing', 'wherever', 'move', 'around', 'against', 'several', 'whither', 'noone', 'what', 'couldnt', 'call', 'most', 'anyway', 'else', 'their', 'your', 'whether', 'one', 'ie', 'toward', 'fifty', 'hereupon', 'eleven', 'they'})\n"
     ]
    }
   ],
   "source": [
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Further study: \n",
    "\n",
    "Link Tutorial: https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/      \n",
    "               http://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/            \n",
    "Punctuation Tokenizer:            \n",
    "               https://sirinnes.wordpress.com/2015/01/22/custom-vectorizer-for-scikit-learn/        \n",
    "Link Doc: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html     \n",
    "          http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "* The output of this program is ment to be fed to an algorithm (i.e KMeans,knn,naive bayes etc..) for further process.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
