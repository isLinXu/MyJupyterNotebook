# Stochastic Gradient Descent (SGD Classifier)
Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers.

SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing.

* The advantages of Stochastic Gradient Descent are:

     * Efficiency.
     * Ease of implementation (lots of opportunities for code tuning).

* The disadvantages of Stochastic Gradient Descent include:

     * SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.
     * SGD is sensitive to feature scaling.
